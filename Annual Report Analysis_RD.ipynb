{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2 \n",
    "import textract\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]\n"
     ]
    }
   ],
   "source": [
    "##write a for loop to open many files\n",
    "# path = r'/root/Desktop/temp_dir'     #path of folder containing several PDFs\n",
    "# for fp in os.listdir(path):\n",
    "#     pdfFileObj = open(os.path.join(path, fp), 'rb')\n",
    "\n",
    "#to parse one file:\n",
    "filename = 'Walmart-AR19.pdf' \n",
    "\n",
    "pdfFileObj = open(filename,'rb')\n",
    "\n",
    "#The pdfReader variable is a readable object that will be parsed\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "\n",
    "#discerning the number of pages will allow us to parse through all the pages\n",
    "num_pages = pdfReader.numPages\n",
    "count = 0\n",
    "text = \"\"\n",
    "\n",
    "while count < num_pages:\n",
    "    pageObj = pdfReader.getPage(count)\n",
    "    count +=1\n",
    "    text += pageObj.extractText()\n",
    "    \n",
    "#This if statement exists to check if the above library returned words. It's done because PyPDF2 cannot read scanned files.\n",
    "if text != \"\":\n",
    "    text = text\n",
    "#If the above returns as False, we run the OCR library textract to #convert scanned/image based PDF files into text\n",
    "else:\n",
    "    text = textract.process(fileurl, method='tesseract', language='eng')\n",
    "    \n",
    "#text variable contains all the text derived from our PDF file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize into individual words\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "punctuations = ['(',')',';',':','[',']',',','.',\"''\",'$','%']\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "#Return a list of words that are NOT IN stop_words and NOT IN punctuations.\n",
    "keywords = [word for word in tokens if not word in stop_words and not word in punctuations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 Sentence Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = {}  \n",
    "for word in word_tokenize(text):  \n",
    "    if word not in stop_words:\n",
    "        if word not in word_frequencies.keys():\n",
    "            word_frequencies[word] = 1\n",
    "        else:\n",
    "            word_frequencies[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_frequency = max(word_frequencies.values())\n",
    "\n",
    "for word in word_frequencies.keys():  \n",
    "    word_frequencies[word] = (word_frequencies[word]/maximum_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4)Central America unit counts by country are Costa Rica (256), El Salvador (97), Guatemala (250), Honduras (105) and Nicaragua (103). Walmart U.S. competes with omni-channel retailers operating discount, department, retail and wholesale grocers, \n",
      "drug, dollar, variety and specialty stores, supermarkets, hypermarkets and supercenter-type stores, as well as eCommerce \n",
      "retailers. We utilize a total of 226 distribution facilities located in Argentina, Canada, Central America, Chile, China,\n",
      "Japan, Mexico, South Africa, India and the United Kingdom. Walmart U.S. provides an omni-channel experience to customers, integrating retail stores and eCommerce, \n",
      "through services such as \"Walmart Pickup,\" \"Pickup Today\", \"Grocery Pickup\", \"Grocery Delivery,\" and \"Endless Aisle.\" Walmart International unit counts, with the exception of Canada, are as of December 31, to\n",
      "correspond with the fiscal year end of the related geographic market. Our discussion is as of and for the fiscal years ended January•31, 2019 (\"fiscal \n",
      "2019\"), January•31, 2018 (\"fiscal 2018\") and January•31, 2017 (\"fiscal 2017\"). Our information\n",
      "systems are essential to our business operations, including the processing of transactions, management of our associates,\n",
      "facilities, logistics, inventories, physical stores and clubs and our online operations. Self Insurance ReservesThe Company self-insures a number of risks, including, but not limited to, workers' compensation, general liability, auto\n",
      "liability, product liability and certain employee-related healthcare benefits. We face strong sales\n",
      "competition from other discount, department, drug, dollar, variety and specialty stores, warehouse clubs and supermarkets, as\n",
      "well as eCommerce businesses. We also utilize third-party service providers for a variety of reasons,\n",
      "\n",
      "including, without limitation, for encryption and authentication technology, content delivery to customers and members, back-\n",
      "\n",
      "office support, and other functions.\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "sentence_list = sent_tokenize(text)  \n",
    "\n",
    "sentence_scores = {}  \n",
    "for sent in sentence_list:  \n",
    "    for word in word_tokenize(sent.lower()):\n",
    "        if word in word_frequencies.keys():\n",
    "            if len(sent.split(' ')) < 30:\n",
    "                if sent not in sentence_scores.keys():\n",
    "                    sentence_scores[sent] = word_frequencies[word]\n",
    "                else:\n",
    "                    sentence_scores[sent] += word_frequencies[word]\n",
    "\n",
    "summary_sentences = heapq.nlargest(10, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "summary = ' '.join(summary_sentences)  \n",
    "print(summary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
