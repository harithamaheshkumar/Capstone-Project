{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary as of 27 Jan \n",
    "- I tried different package to read pdf, and pdftotext works best for the transcripts files\n",
    "- There are two format types. One is fancier. \n",
    "- My next step is to parse the fancier files.\n",
    "\n",
    "### Summary as of 1st Feb\n",
    "**Parse fancy files**\n",
    "\n",
    "- the basic unit: each paragraph spoken by one person (i.e. the content divided by dotted line)\n",
    "- clean those words that are repeated and obviously useless\n",
    "- Words appeared in the front page and CORPORATE PARTICIPANTS session are repeated and contain no information\n",
    "\n",
    "**Next step**\n",
    "\n",
    "- topic modeling\n",
    "- optimize text preprocessing (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdftotext\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def is_fancy(filepath):\n",
    "    check_words = [\"Corrected Transcript\",\n",
    "                  \"www.callstreet.com\",\n",
    "                  \"FactSet CallStreet, LLC\"]\n",
    "    pdfFileObj = open(filepath,'rb')\n",
    "    pdf = pdftotext.PDF(pdfFileObj)\n",
    "    text = \"\\n\\n\".join(pdf)\n",
    "    if all([ check_word in text for check_word in check_words]):\n",
    "        return(\"fancy\")\n",
    "    else:\n",
    "        return(\"plain\")\n",
    "    \n",
    "def get_fp_cp(fp):\n",
    "    output = list()\n",
    "    for item in fp[fp.index(\"CORPORATE PARTICIPANTS\"):]:\n",
    "        if \"Walmart\" in item:\n",
    "            for x in item.split(\".\"):\n",
    "                output = output + x.split(\",\")\n",
    "            for x in item.split(\"-\"):\n",
    "                output = output + x.split(\"   \")\n",
    "            for x in item.split(\"-\"):\n",
    "                output = output + x.split(\".\")\n",
    "            for x in item.split(\"   \"):\n",
    "                output = output + x.split(\",\")\n",
    "        else: \n",
    "            output = output + item.split()\n",
    "    return([item.strip() for item in output if len(item) > 1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd() + '\\data\\Transcripts2019'\n",
    "files = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20190219_earningcall_2019Q4.pdf</td>\n",
       "      <td>fancy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20190305_conference.pdf</td>\n",
       "      <td>plain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20190306_conference.PDF</td>\n",
       "      <td>fancy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20190312_conference.pdf</td>\n",
       "      <td>fancy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20190516_earningcall_2020Q1.pdf</td>\n",
       "      <td>plain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20190530_conference.pdf</td>\n",
       "      <td>fancy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20190607_investmentcommunity.pdf</td>\n",
       "      <td>plain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20190611_conference.pdf</td>\n",
       "      <td>fancy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20190815_earningcall_2020Q2.pdf</td>\n",
       "      <td>plain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20190904_conference.pdf</td>\n",
       "      <td>fancy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20190905_conference.pdf</td>\n",
       "      <td>fancy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20191114_earningcall_2020Q3.pdf</td>\n",
       "      <td>plain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20191115_conference.pdf</td>\n",
       "      <td>fancy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20191204_conference.pdf</td>\n",
       "      <td>fancy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            filename   type\n",
       "0    20190219_earningcall_2019Q4.pdf  fancy\n",
       "1            20190305_conference.pdf  plain\n",
       "2            20190306_conference.PDF  fancy\n",
       "3            20190312_conference.pdf  fancy\n",
       "4    20190516_earningcall_2020Q1.pdf  plain\n",
       "5            20190530_conference.pdf  fancy\n",
       "6   20190607_investmentcommunity.pdf  plain\n",
       "7            20190611_conference.pdf  fancy\n",
       "8    20190815_earningcall_2020Q2.pdf  plain\n",
       "9            20190904_conference.pdf  fancy\n",
       "10           20190905_conference.pdf  fancy\n",
       "11   20191114_earningcall_2020Q3.pdf  plain\n",
       "12           20191115_conference.pdf  fancy\n",
       "13           20191204_conference.pdf  fancy"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = pd.DataFrame(files, columns=[\"filename\"])\n",
    "files[\"type\"] = files[\"filename\"].map(lambda x: is_fancy(path + \"\\\\\" + x))\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse fancy files\n",
    "- the basic unit: each paragraph spoken by one person (i.e. the content divided by dotted line)\n",
    "- clean those words that are repeated and obviously useless\n",
    "- Words appeared in the front page and CORPORATE PARTICIPANTS session are repeated and contain no information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_fancy_para(filepath):\n",
    "    \"\"\" return a dataframe, each row is a paragraph spoken by one person\n",
    "    \"\"\" \n",
    "    pdfFileObj = open(filepath,'rb')\n",
    "    pdf = pdftotext.PDF(pdfFileObj)\n",
    "    text = \"\\n\\n\".join(pdf)\n",
    "\n",
    "    tmp = text.split(\"........................................\\\n",
    "..............................................................\\\n",
    "..............................................................\\\n",
    "..................................................................................\")\n",
    "\n",
    "    # process words appeared in the front page and CORPORATE PARTICIPANTS section\n",
    "    fp = [item.strip() for item in tmp[0].split(\"\\r\\n\") if len(item)>=1]\n",
    "\n",
    "    to_be_removed = [fp[0], fp[1], fp[3]] + \\\n",
    "                    fp[2].split() + \\\n",
    "                    [x for x in fp[5].split(\" \") if len(x) >= 1] + \\\n",
    "                    get_fp_cp(fp)\n",
    "\n",
    "    to_be_removed_df = pd.DataFrame(set(to_be_removed))\n",
    "    to_be_removed_df[\"len\"] = to_be_removed_df[0].map(lambda x: len(x))\n",
    "    to_be_removed_df = to_be_removed_df.sort_values([\"len\"], ascending = False)\n",
    "\n",
    "    for item in to_be_removed_df[0]:\n",
    "        for i in range(1, len(tmp)):\n",
    "            tmp[i] = tmp[i].replace(item, '')\n",
    "            tmp[i] = tmp[i].replace(\"\\r\\n\", '').lstrip(\", \")\n",
    "\n",
    "    return(pd.DataFrame(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "fancy = files[ files[\"type\"] == \"fancy\" ]\n",
    "\n",
    "test = pd.DataFrame()\n",
    "for filename in fancy[\"filename\"]:\n",
    "    filepath = path + \"\\\\\" + filename\n",
    "    test = pd.concat([test, parse_fancy_para(filepath)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OTHER Simeon Ari Gutman                       ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thank you. Good morning and welcome to 's four...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good morning, everyone, and thanks for joining...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You bet. Thanks, Doug. Good morning, everybody...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>QUESTION AND ANSWER SECTIONOperator: Thank you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Simeon Ari GutmanAnalyst, Morgan Stanley &amp; Co....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AYeah, thanks, Simeon. We've talked a lot over...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10                                            ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AYeah, Simeon, this is Doug. I think the headl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Operator: Thank you. The next question is from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Robert DrbulAnalyst, Guggenheim Securities    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AHi, Bob.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Robert DrbulAnalyst, Guggenheim Securities    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>AYeah, Bob, this is Doug. I'm more confident i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Robert DrbulAnalyst, Guggenheim Securities    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>AI can't comment on the future, the guidance f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Robert DrbulAnalyst, Guggenheim Securities    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Operator: The next question is coming from the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Peter S. BenedictAnalyst, Robert W. Baird &amp; Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ASure. If I can take you back to the October A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Peter S. BenedictAnalyst, Robert W. Baird &amp; Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>AYeah, you bet. Thanks, Peter. Yeah, traffic a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Operator: Thank you. The next question comes f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Kelly Ann BaniaAnalyst, BMO Capital Markets (U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>AYeah, this is Doug, Kelly. I think it's going...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>AAnd, Kelly, this is . As we said in October, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Kelly Ann BaniaAnalyst, BMO Capital Markets (U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>AI don't think we should get too specific by c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>AAnd our merchants, it changes every year, but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>Karen ShortAnalyst, Barclays Capital,         ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>AYeah. So the simplistic  productivity loop, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>Karen ShortAnalyst, Barclays Capital,         ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>AIn the end, there's at least kind of three co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>Karen ShortAnalyst, Barclays Capital,         ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>AYeah. It is. I think it's unclear to me how t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>Karen ShortAnalyst, Barclays Capital,         ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>AYeah. I think it's true in Sam's too, but in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>Karen ShortAnalyst, Barclays Capital,         ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>A                                             ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>Karen ShortAnalyst, Barclays Capital,         ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>QCan you touch on just your thoughts of organi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>AYeah. I think we have a lot of opportunity th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>Karen ShortAnalyst, Barclays Capital,         ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>QDoug, can I ask a question on – I'm intrigued...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>AI don't think of it as tension. I just think ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>Karen ShortAnalyst, Barclays Capital,         ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>AYeah. It's going to be really fun to watch an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>Karen ShortAnalyst, Barclays Capital,         ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>ASomehow we'll figure out how to expand even b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>Karen ShortAnalyst, Barclays Capital,         ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>AAre you in a hurry for that to happen?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>Karen ShortAnalyst, Barclays Capital,         ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>AI was just thinking about that very much. Do ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>Karen ShortAnalyst, Barclays Capital,         ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>AMe too. I think it's important for us all to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>Karen ShortAnalyst, Barclays Capital, I think ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>Thanks for having me.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>Karen ShortAnalyst, Barclays Capital, Yeah.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>Thank you, all.   Disclaimer   The information...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>978 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0\n",
       "0                                                  ...\n",
       "1    OTHER Simeon Ari Gutman                       ...\n",
       "2    Thank you. Good morning and welcome to 's four...\n",
       "3    Good morning, everyone, and thanks for joining...\n",
       "4    You bet. Thanks, Doug. Good morning, everybody...\n",
       "5    QUESTION AND ANSWER SECTIONOperator: Thank you...\n",
       "6    Simeon Ari GutmanAnalyst, Morgan Stanley & Co....\n",
       "7    AYeah, thanks, Simeon. We've talked a lot over...\n",
       "8    10                                            ...\n",
       "9    AYeah, Simeon, this is Doug. I think the headl...\n",
       "10   Operator: Thank you. The next question is from...\n",
       "11   Robert DrbulAnalyst, Guggenheim Securities    ...\n",
       "12                                           AHi, Bob.\n",
       "13   Robert DrbulAnalyst, Guggenheim Securities    ...\n",
       "14   AYeah, Bob, this is Doug. I'm more confident i...\n",
       "15   Robert DrbulAnalyst, Guggenheim Securities    ...\n",
       "16   AI can't comment on the future, the guidance f...\n",
       "17   Robert DrbulAnalyst, Guggenheim Securities    ...\n",
       "18   Operator: The next question is coming from the...\n",
       "19   Peter S. BenedictAnalyst, Robert W. Baird & Co...\n",
       "20   ASure. If I can take you back to the October A...\n",
       "21   Peter S. BenedictAnalyst, Robert W. Baird & Co...\n",
       "22   AYeah, you bet. Thanks, Peter. Yeah, traffic a...\n",
       "23   Operator: Thank you. The next question comes f...\n",
       "24   Kelly Ann BaniaAnalyst, BMO Capital Markets (U...\n",
       "25   AYeah, this is Doug, Kelly. I think it's going...\n",
       "26   AAnd, Kelly, this is . As we said in October, ...\n",
       "27   Kelly Ann BaniaAnalyst, BMO Capital Markets (U...\n",
       "28   AI don't think we should get too specific by c...\n",
       "29   AAnd our merchants, it changes every year, but...\n",
       "..                                                 ...\n",
       "948  Karen ShortAnalyst, Barclays Capital,         ...\n",
       "949  AYeah. So the simplistic  productivity loop, w...\n",
       "950  Karen ShortAnalyst, Barclays Capital,         ...\n",
       "951  AIn the end, there's at least kind of three co...\n",
       "952  Karen ShortAnalyst, Barclays Capital,         ...\n",
       "953  AYeah. It is. I think it's unclear to me how t...\n",
       "954  Karen ShortAnalyst, Barclays Capital,         ...\n",
       "955  AYeah. I think it's true in Sam's too, but in ...\n",
       "956  Karen ShortAnalyst, Barclays Capital,         ...\n",
       "957  A                                             ...\n",
       "958  Karen ShortAnalyst, Barclays Capital,         ...\n",
       "959  QCan you touch on just your thoughts of organi...\n",
       "960  AYeah. I think we have a lot of opportunity th...\n",
       "961  Karen ShortAnalyst, Barclays Capital,         ...\n",
       "962  QDoug, can I ask a question on – I'm intrigued...\n",
       "963  AI don't think of it as tension. I just think ...\n",
       "964  Karen ShortAnalyst, Barclays Capital,         ...\n",
       "965  AYeah. It's going to be really fun to watch an...\n",
       "966  Karen ShortAnalyst, Barclays Capital,         ...\n",
       "967  ASomehow we'll figure out how to expand even b...\n",
       "968  Karen ShortAnalyst, Barclays Capital,         ...\n",
       "969            AAre you in a hurry for that to happen?\n",
       "970  Karen ShortAnalyst, Barclays Capital,         ...\n",
       "971  AI was just thinking about that very much. Do ...\n",
       "972  Karen ShortAnalyst, Barclays Capital,         ...\n",
       "973  AMe too. I think it's important for us all to ...\n",
       "974  Karen ShortAnalyst, Barclays Capital, I think ...\n",
       "975                              Thanks for having me.\n",
       "976        Karen ShortAnalyst, Barclays Capital, Yeah.\n",
       "977  Thank you, all.   Disclaimer   The information...\n",
       "\n",
       "[978 rows x 1 columns]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is working space: trying to optimize deletion of CORPORATE PARTICIPANTS section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "fancy = files[ files[\"type\"] == \"fancy\" ]\n",
    "filename = fancy[\"filename\"].iloc[1]\n",
    "filepath = path + \"\\\\\" + filename\n",
    "\n",
    "pdfFileObj = open(filepath,'rb')\n",
    "pdf = pdftotext.PDF(pdfFileObj)\n",
    "text = \"\\n\\n\".join(pdf)\n",
    "\n",
    "tmp = text.split(\"........................................\\\n",
    "..............................................................\\\n",
    "..............................................................\\\n",
    "..................................................................................\")\n",
    "\n",
    "# process words appeared in the front page and CORPORATE PARTICIPANTS section\n",
    "fp = [item.strip() for item in tmp[0].split(\"\\r\\n\") if len(item)>=1]\n",
    "\n",
    "to_be_removed = [fp[0], fp[1], fp[3]] + \\\n",
    "                fp[2].split() + \\\n",
    "                [x for x in fp[5].split(\" \") if len(x) >= 1] + \\\n",
    "                get_fp_cp(fp)\n",
    "\n",
    "to_be_removed_df = pd.DataFrame(set(to_be_removed))\n",
    "to_be_removed_df[\"len\"] = to_be_removed_df[0].map(lambda x: len(x))\n",
    "to_be_removed_df = to_be_removed_df.sort_values([\"len\"], ascending = False)\n",
    "\n",
    "for item in to_be_removed_df[0]:\n",
    "    for i in range(1, len(tmp)):\n",
    "        tmp[i] = tmp[i].replace(item, '')\n",
    "        tmp[i] = tmp[i].replace(\"\\r\\n\", '').lstrip(\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5     UBS Securities LLC                            ...\n",
       "10    Analyst, UBS Securities LLC                   ...\n",
       "13       President & Chief Executive Officer-Walmart US\n",
       "25             Chief Financial Officer-Walmart US & EVP\n",
       "20              UBS Global Consumer & Retail Conference\n",
       "36                  President & Chief Executive Officer\n",
       "0                       Walmart US & EVP, Walmart, Inc.\n",
       "1                        Walmart US & EVP, Walmart, Inc\n",
       "23                          Analyst, UBS Securities LLC\n",
       "7                             Walmart US, Walmart, Inc.\n",
       "6                              Walmart US, Walmart, Inc\n",
       "14                              Chief Financial Officer\n",
       "33                                 Corrected Transcript\n",
       "35                                   www.callstreet.com\n",
       "32                                   UBS Securities LLC\n",
       "4                                         1-877-FACTSET\n",
       "2                                          PARTICIPANTS\n",
       "11                                          06-Mar-2019\n",
       "29                                          CallStreet,\n",
       "30                                            Copyright\n",
       "37                                            2001-2019\n",
       "16                                            CORPORATE\n",
       "18                                             Dastugue\n",
       "31                                             Walmart,\n",
       "8                                               Walmart\n",
       "34                                              Analyst\n",
       "28                                              Michael\n",
       "22                                              Gregory\n",
       "12                                              FactSet\n",
       "15                                               Lasser\n",
       "9                                                 (WMT)\n",
       "24                                                Foran\n",
       "3                                                  Inc.\n",
       "21                                                  LLC\n",
       "17                                                  Inc\n",
       "27                                                   P.\n",
       "26                                                   S.\n",
       "19                                                    ©\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_be_removed_df = pd.DataFrame(set(to_be_removed))\n",
    "to_be_removed_df[\"len\"] = to_be_removed_df[0].map(lambda x: len(x))\n",
    "to_be_removed_df = to_be_removed_df.sort_values([\"len\"], ascending = False)\n",
    "to_be_removed_df[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
